{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "import numpy as np \n",
    "import pandas as pd \n",
    "import torch \n",
    "import scipy as sc\n",
    "import stan\n",
    "from src.df_processing import * \n",
    "import edward as ed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define your function nu(mu, omega)\n",
    "def nu_computation(mu, omega, tau):\n",
    "    # Define your function here using mu and omega\n",
    "    nu = np.linalg.inv(np.diag(np.exp(omega)))*(tau - mu)\n",
    "    return nu  # Example function, replace with your actual function\n",
    "\n",
    "def objective_fcts(): \n",
    "    # E_{N(0,I)}[∇_{theta}logp(x,θ)∇ T−1(ζ)+∇ log detJ −1(ζ)\u0003\n",
    "\n",
    "    return fct_mu, fct_omega\n",
    "\n",
    "def step_size(): \n",
    "    return \n",
    "\n",
    "def MC_integration(fct_obj, tirage, nu, nb_samples): \n",
    "    somme = 0\n",
    "    for i in range(nb_samples): \n",
    "        nu_s = tirage(nu)\n",
    "        somme += fct_obj(nu_s)\n",
    "    return (1/nb_samples)*somme"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ppca(X, d):\n",
    "    \"\"\"\n",
    "    Probabilistic Principal Component Analysis (PPCA).\n",
    "    \n",
    "    Parameters:\n",
    "        X (numpy.ndarray): Input data matrix of shape (N, M), where N is the number of samples and M is the number of features.\n",
    "        d (int): Number of principal components.\n",
    "        \n",
    "    Returns:\n",
    "        tuple: Tuple containing the reconstructed data matrix, the projection matrix, and the noise variance.\n",
    "    \"\"\"\n",
    "    N, M = X.shape\n",
    "\n",
    "    # Center the data\n",
    "    X_mean = np.mean(X, axis=0)\n",
    "    X_centered = X - X_mean\n",
    "\n",
    "    # Estimate covariance matrix\n",
    "    cov_X = np.cov(X_centered, rowvar=False)\n",
    "\n",
    "    # Perform eigenvalue decomposition\n",
    "    eigenvalues, eigenvectors = np.linalg.eigh(cov_X)\n",
    "\n",
    "    # Select the top d eigenvectors\n",
    "    W = eigenvectors[:, -d:]\n",
    "\n",
    "    # Estimate the noise variance\n",
    "    sigma_squared = 1/(M - d) * np.sum(eigenvalues[:-d])\n",
    "\n",
    "    # Compute the projection matrix\n",
    "    M_inv = np.linalg.inv(np.dot(W.T, W) + sigma_squared * np.eye(d))\n",
    "    C = np.dot(np.dot(W, M_inv), W.T)\n",
    "\n",
    "    # Reconstruct the data\n",
    "    X_reconstructed = np.dot(np.dot(C, X_centered.T), W.T) + X_mean.reshape(1, -1)\n",
    "\n",
    "    return X_reconstructed.T, W, sigma_squared"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model definition\n",
    "def probas(N, M, D):\n",
    "    # Priors\n",
    "    z = sc.stats.multivariate_normal.rvs(mean=np.zeros(M), cov=np.eye(M, M), size=N)\n",
    "    proba_z = sc.stats.multivariate_normal.pdf(z, mean=np.zeros(M), cov=np.eye(M, M)).prod()\n",
    "    print(\"p_z: \",proba_z)\n",
    "\n",
    "    alpha = sc.stats.invgamma.rvs(a=1,size=M)\n",
    "    proba_alpha = sc.stats.invgamma.pdf(alpha, a=1).prod()\n",
    "    print(\"p_alpha: \",proba_alpha)\n",
    "\n",
    "    sigma = sc.stats.lognorm.rvs(loc=0, s=1, size=1)[0]\n",
    "    proba_sigma = sc.stats.lognorm.pdf(sigma, loc=0, s=1)\n",
    "    print(\"p_sigma: \",proba_sigma)\n",
    "\n",
    "    w = sc.stats.multivariate_normal.rvs(mean=np.zeros(M), cov=sigma*np.diag(alpha), size=D)\n",
    "    proba_w = sc.stats.multivariate_normal.pdf(w, mean=np.zeros(M), cov=sigma*np.diag(alpha)).prod()\n",
    "    print(\"proba_w: \",proba_w)\n",
    "\n",
    "    # likelihood\n",
    "    lik = []\n",
    "    proba_lik =1\n",
    "    for i in range(N):\n",
    "        obs = sc.stats.multivariate_normal.rvs(mean=np.dot(w,z[i]), cov=sigma*np.eye(D, D), size=1)         \n",
    "        lik.append(obs)\n",
    "        proba_lik *= sc.stats.multivariate_normal.pdf(obs, mean=np.dot(w,z[i]), cov=sigma*np.eye(D, D))\n",
    "\n",
    "    print(\"proba_lik: \",proba_lik)\n",
    "    priors = [z,alpha,sigma,w, lik]\n",
    "    proba_priors = [proba_z, proba_alpha, proba_sigma, proba_w, proba_lik]\n",
    "\n",
    "    #joint distribution \n",
    "    p_theta = proba_z*proba_alpha*proba_sigma*proba_w\n",
    "    jd = p_theta*proba_lik\n",
    "    print(\"p_jd: \",jd)\n",
    "\n",
    "    return jd, priors, proba_priors\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "p_z:  3.8858959628814784e-29\n",
      "p_alpha:  0.0009597463589742657\n",
      "p_sigma:  0.4874823900544616\n",
      "proba_w:  1.561470073152198e-12\n",
      "proba_lik:  2.975586420547798e-32\n",
      "p_jd:  8.447200588225281e-76\n"
     ]
    }
   ],
   "source": [
    "D = 5 # dimension \n",
    "M = 4 # maximum dimensions of latent space to consider \n",
    "N = 10 # number of data points in dataset \n",
    "jd, priors, proba_priors = probas(N, M, D)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ADVI(x, ): \n",
    "    # Dataset x(1:N)\n",
    "    N = len(x)\n",
    "    # Model p(x,θ)\n",
    "    i = 1 # Set iteration counter \n",
    "\n",
    "    # Parameter initialization \n",
    "    mu = torch.zeros(size= N, requires_grad=True)\n",
    "    omega = torch.zeros(size= N, requires_grad=True) # Mean-field\n",
    "\n",
    "    nu = torch.linalg.inv(torch.diag(torch.exp(w)))@(tau-mu)\n",
    "    thr =  1e-4\n",
    "    change_in_ELBO = 100\n",
    "    # Define optimizer\n",
    "    optimizer = torch.optim.SGD([mu, omega], lr=0.01)\n",
    "\n",
    "    while True: \n",
    "        # Draw M samples from normal stamdard multivariate gaussian\n",
    "        # Approximate gradients (wrt mu and w) with MC integration \n",
    "        # Compute nu\n",
    "        nu = nu_computation(mu, omega, tau)\n",
    "\n",
    "        L = objective_fct(nu, tau)\n",
    "\n",
    "        # Backpropagation to compute gradients\n",
    "        optimizer.zero_grad()\n",
    "        L.backward()\n",
    "\n",
    "        # Get gradients with respect to mu and omega\n",
    "        gradient_mu = mu.grad\n",
    "        gradient_omega = omega.grad\n",
    "        \n",
    "        # Update parameters\n",
    "        optimizer.step()\n",
    "\n",
    "        # Calculate step-size rho[i]\n",
    "        rho = step_size(i, nu, rho)\n",
    "\n",
    "        # Update mu and w \n",
    "        mu += np.diag(rho) @ gradient_mu\n",
    "        omega += np.diag(rho) @ gradient_omega\n",
    "        change_in_ELBO = np.abs()\n",
    "\n",
    "        # increment iteration counter\n",
    "        i +=1\n",
    "        if abs(change_in_ELBO) < thr:\n",
    "            break \n",
    "\n",
    "    return mu.item(), omega.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "optimizer got an empty parameter list",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/30/cz4hwnjn49sbv5x54df4kt4m0000gn/T/ipykernel_65854/2272006141.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;31m# Define your optimizer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m \u001b[0moptimizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moptim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSGD\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.01\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;31m# Main optimization loop\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/torch/optim/sgd.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, params, lr, momentum, dampening, weight_decay, nesterov, maximize, foreach, differentiable)\u001b[0m\n\u001b[1;32m    107\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mnesterov\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mmomentum\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mdampening\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    108\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Nesterov momentum requires a momentum and zero dampening\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 109\u001b[0;31m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mSGD\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdefaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    110\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    111\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__setstate__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/torch/optim/optimizer.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, params, defaults)\u001b[0m\n\u001b[1;32m     59\u001b[0m         \u001b[0mparam_groups\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparam_groups\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 61\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"optimizer got an empty parameter list\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     62\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparam_groups\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m             \u001b[0mparam_groups\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m'params'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mparam_groups\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: optimizer got an empty parameter list"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.optim as optim\n",
    "\n",
    "# Define your model using PyTorch\n",
    "class Model(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Model, self).__init__()\n",
    "        # Define your model architecture here\n",
    "    \n",
    "    def forward(self, x, theta):\n",
    "        # Define the forward pass of your model\n",
    "        return output\n",
    "\n",
    "# Initialize your model\n",
    "model = Model()\n",
    "\n",
    "# Define your loss function\n",
    "criterion = torch.nn.MSELoss()\n",
    "\n",
    "# Define your optimizer\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.01)\n",
    "\n",
    "# Main optimization loop\n",
    "threshold = 1e-6\n",
    "while True:\n",
    "    # Forward pass\n",
    "    output = model(x, theta)\n",
    "    \n",
    "    # Compute the loss\n",
    "    loss = criterion(output, target)\n",
    "    \n",
    "    # Backward pass\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    \n",
    "    # Update parameters\n",
    "    optimizer.step()\n",
    "    \n",
    "    # Check convergence\n",
    "    if abs(change_in_loss) < threshold:\n",
    "        break\n",
    "\n",
    "# Get the optimal values\n",
    "optimal_values = model.parameters()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "pystan_code = \"\"\"\n",
    "data { \n",
    "    int<lower=0> N; // number of data points in dataset\n",
    "    int<lower=0> D; // dimension\n",
    "    int<lower=0> M; // maximum dimension of latent space to consider\n",
    "    vector[D] x[N]; // data\n",
    "}\n",
    "\n",
    "parameters {\n",
    "    matrix[M, N] z; // latent variable\n",
    "    matrix[D, M] w; // weights parameters\n",
    "    real<lower=0> sigma; // standard deviation parameter\n",
    "    vector<lower=0>[M] alpha; / hyper-parameters on weights\n",
    "}\n",
    "\n",
    "model {\n",
    "// priors\n",
    "    to_vector(z) ~ normal(0, 1); \n",
    "    for (d in 1:D)\n",
    "        w[d] ~ normal(0, sigma * alpha);\n",
    "    sigma ~ lognormal(0, 1);\n",
    "    alpha ~ inv_gamma(1, 1);\n",
    "    \n",
    "    // likelihood\n",
    "    for (n in 1:N)\n",
    "        x[n] ~ normal(w * col(z, n), sigma);\n",
    "}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('../taxi+service+trajectory+prediction+challenge+ecml+pkdd+2015/interpolation200.csv')\n",
    "\n",
    "ls = [i.tolist() for i in extract_traj(df)]\n",
    "\n",
    "code_data = {\"N\": len(ls),\n",
    "                \"D\": 5,\n",
    "                \"M\": 2,\n",
    "                \"x[N]\": [[1,3,4,6,3] for i in range(len(ls))]}"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
