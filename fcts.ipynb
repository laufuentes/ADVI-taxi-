{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd \n",
    "import torch \n",
    "import scipy as sc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define your function nu(mu, omega)\n",
    "def nu_computation(mu, omega, tau):\n",
    "    # Define your function here using mu and omega\n",
    "    nu = np.linalg.inv(np.diag(np.exp(omega)))*(tau - mu)\n",
    "    return nu  # Example function, replace with your actual function\n",
    "\n",
    "def objective_fct(): \n",
    "    return \n",
    "\n",
    "def step_size(): \n",
    "    return \n",
    "\n",
    "def MC_integration(fct_obj, tirage, nu, nb_samples): \n",
    "    somme = 0\n",
    "    for i in range(nb_samples): \n",
    "        nu_s = tirage(nu)\n",
    "        somme += fct_obj(nu_s)\n",
    "    return (1/nb_samples)*somme"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ppca(X, d):\n",
    "    \"\"\"\n",
    "    Probabilistic Principal Component Analysis (PPCA).\n",
    "    \n",
    "    Parameters:\n",
    "        X (numpy.ndarray): Input data matrix of shape (N, M), where N is the number of samples and M is the number of features.\n",
    "        d (int): Number of principal components.\n",
    "        \n",
    "    Returns:\n",
    "        tuple: Tuple containing the reconstructed data matrix, the projection matrix, and the noise variance.\n",
    "    \"\"\"\n",
    "    N, M = X.shape\n",
    "\n",
    "    # Center the data\n",
    "    X_mean = np.mean(X, axis=0)\n",
    "    X_centered = X - X_mean\n",
    "\n",
    "    # Estimate covariance matrix\n",
    "    cov_X = np.cov(X_centered, rowvar=False)\n",
    "\n",
    "    # Perform eigenvalue decomposition\n",
    "    eigenvalues, eigenvectors = np.linalg.eigh(cov_X)\n",
    "\n",
    "    # Select the top d eigenvectors\n",
    "    W = eigenvectors[:, -d:]\n",
    "\n",
    "    # Estimate the noise variance\n",
    "    sigma_squared = 1/(M - d) * np.sum(eigenvalues[:-d])\n",
    "\n",
    "    # Compute the projection matrix\n",
    "    M_inv = np.linalg.inv(np.dot(W.T, W) + sigma_squared * np.eye(d))\n",
    "    C = np.dot(np.dot(W, M_inv), W.T)\n",
    "\n",
    "    # Reconstruct the data\n",
    "    X_reconstructed = np.dot(np.dot(C, X_centered.T), W.T) + X_mean.reshape(1, -1)\n",
    "\n",
    "    return X_reconstructed.T, W, sigma_squared"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model definition\n",
    "def probas(N, M, D):\n",
    "    # Priors\n",
    "    z = sc.stats.multivariate_normal.rvs(mean=np.zeros(M), cov=np.eye(M, M), size=N)\n",
    "    proba_z = np.prod(z, axis=1)\n",
    "\n",
    "    alpha = sc.stats.invgamma.rvs(1, size=M)\n",
    "    proba_alpha = np.prod(alpha)\n",
    "\n",
    "    sigma = sc.stats.lognorm.rvs(loc=0, s=1, size=1)[0]\n",
    "\n",
    "    w = sc.stats.multivariate_normal.rvs(mean=np.zeros(M), cov=sigma*np.diag(alpha), size=D)\n",
    "    proba_w = np.prod(w, axis=1)\n",
    "\n",
    "    # likelihood\n",
    "    lik = []\n",
    "    for i in range(N):         \n",
    "        lik.append(sc.stats.multivariate_normal.rvs(mean=np.dot(w,z[i]), cov=sigma*np.eye(D, D), size=1) )\n",
    "    dd = np.prod(lik, axis=1)\n",
    "\n",
    "    priors = np.array([z,alpha,sigma,w, lik])\n",
    "    proba_priors = np.array([proba_z, proba_alpha, sigma, proba_w, prob_lik])\n",
    "\n",
    "    #joint distribution \n",
    "    jd = proba_z*proba_alpha*sigma*proba_w*prob_lik\n",
    "\n",
    "    return jd, priors, proba_priors\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "D = 5 # dimension \n",
    "M = 4 # maximum dimensions of latent space to consider \n",
    "N = 10 # number of data points in dataset \n",
    "#probas(N, M, D)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ADVI(x, ): \n",
    "    # Dataset x(1:N)\n",
    "    N = len(x)\n",
    "    # Model p(x,Î¸)\n",
    "    i = 1 # Set iteration counter \n",
    "\n",
    "    # Parameter initialization \n",
    "    mu = torch.zeros(size= N, requires_grad=True)\n",
    "    omega = torch.zeros(size= N, requires_grad=True) # Mean-field\n",
    "\n",
    "    nu = torch.linalg.inv(torch.diag(torch.exp(w)))@(tau-mu)\n",
    "    thr =  1e-4\n",
    "    change_in_ELBO = 100\n",
    "    # Define optimizer\n",
    "    optimizer = torch.optim.SGD([mu, omega], lr=0.01)\n",
    "\n",
    "    while True: \n",
    "        # Draw M samples from normal stamdard multivariate gaussian\n",
    "        # Approximate gradients (wrt mu and w) with MC integration \n",
    "        # Compute nu\n",
    "        nu = nu_computation(mu, omega, tau)\n",
    "\n",
    "        L = objective_fct(nu, tau)\n",
    "\n",
    "        # Backpropagation to compute gradients\n",
    "        optimizer.zero_grad()\n",
    "        L.backward()\n",
    "\n",
    "        # Get gradients with respect to mu and omega\n",
    "        gradient_mu = mu.grad\n",
    "        gradient_omega = omega.grad\n",
    "        \n",
    "        # Update parameters\n",
    "        optimizer.step()\n",
    "\n",
    "        # Calculate step-size rho[i]\n",
    "        rho = step_size(i, nu, rho)\n",
    "\n",
    "        # Update mu and w \n",
    "        mu += np.diag(rho) @ gradient_mu\n",
    "        omega += np.diag(rho) @ gradient_omega\n",
    "        change_in_ELBO = np.abs()\n",
    "\n",
    "        # increment iteration counter\n",
    "        i +=1\n",
    "        if abs(change_in_ELBO) < thr:\n",
    "            break \n",
    "\n",
    "    return mu.item(), omega.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.optim as optim\n",
    "\n",
    "# Define your model using PyTorch\n",
    "class Model(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Model, self).__init__()\n",
    "        # Define your model architecture here\n",
    "    \n",
    "    def forward(self, x, theta):\n",
    "        # Define the forward pass of your model\n",
    "        return output\n",
    "\n",
    "# Initialize your model\n",
    "model = Model()\n",
    "\n",
    "# Define your loss function\n",
    "criterion = torch.nn.MSELoss()\n",
    "\n",
    "# Define your optimizer\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.01)\n",
    "\n",
    "# Main optimization loop\n",
    "threshold = 1e-6\n",
    "while True:\n",
    "    # Forward pass\n",
    "    output = model(x, theta)\n",
    "    \n",
    "    # Compute the loss\n",
    "    loss = criterion(output, target)\n",
    "    \n",
    "    # Backward pass\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    \n",
    "    # Update parameters\n",
    "    optimizer.step()\n",
    "    \n",
    "    # Check convergence\n",
    "    if abs(change_in_loss) < threshold:\n",
    "        break\n",
    "\n",
    "# Get the optimal values\n",
    "optimal_values = model.parameters()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
