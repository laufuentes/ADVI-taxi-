{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\nafis\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n",
      "WARNING:tensorflow:From c:\\Users\\nafis\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tensorflow_probability\\python\\internal\\backend\\numpy\\_utils.py:48: The name tf.logging.TaskLevelStatusMessage is deprecated. Please use tf.compat.v1.logging.TaskLevelStatusMessage instead.\n",
      "\n",
      "WARNING:tensorflow:From c:\\Users\\nafis\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tensorflow_probability\\python\\internal\\backend\\numpy\\_utils.py:48: The name tf.control_flow_v2_enabled is deprecated. Please use tf.compat.v1.control_flow_v2_enabled instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import scipy as sc\n",
    "import pandas as pd \n",
    "from pprint import pprint\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow_probability as tfp\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_probability as tfp\n",
    "\n",
    "tfd = tfp.distributions\n",
    "\n",
    "# Define parameters\n",
    "N = 10  # Number of samples\n",
    "M = 4  # Dimensionality latent variables\n",
    "D = 5  # \n",
    "\n",
    "def probas(N, M, D):\n",
    "    # Prior distributions\n",
    "    # z ~ Multivariate Normal distribution\n",
    "    z_prior = tfd.MultivariateNormalFullCovariance(\n",
    "        loc=tf.zeros(M),\n",
    "        covariance_matrix=tf.eye(M))\n",
    "    z = z_prior.sample(N)\n",
    "    proba_z = tf.reduce_prod(z_prior.prob(z)).numpy() # p_z\n",
    "    exp_log_proba_z = tf.reduce_prod(tf.exp(z_prior.log_prob(z))) # exp(log_p_z)\n",
    "    print(\"p_z: \", proba_z, exp_log_proba_z.numpy())\n",
    "\n",
    "    # alpha ~ Inverse Gamma distribution\n",
    "    alpha_prior = tfd.InverseGamma(concentration=1.0, scale=tf.ones(M))\n",
    "    proba_alpha = tf.reduce_prod(alpha_prior.prob(alpha_prior.sample())).numpy()\n",
    "    print(\"p_alpha: \", proba_alpha)\n",
    "\n",
    "    # sigma ~ Log-normal distribution\n",
    "    sigma_prior = tfd.LogNormal(loc=0.0,scale=1.0)\n",
    "    s = sigma_prior.sample()\n",
    "    proba_sigma = sigma_prior.prob(s).numpy()\n",
    "    print(\"p_sigma: \", proba_sigma)\n",
    "\n",
    "    # w ~ Multivariate Normal distribution\n",
    "    w_prior = tfd.MultivariateNormalFullCovariance(loc=tf.zeros(M),covariance_matrix=sigma_prior.sample() * tf.linalg.diag(alpha_prior.sample()))\n",
    "    w = w_prior.sample(D)\n",
    "    proba_w = tf.reduce_prod(w_prior.prob(w)).numpy()\n",
    "    print(\"proba_w: \", proba_w)\n",
    "\n",
    "    lik = []\n",
    "    proba_lik = 1\n",
    "    for i in range(N):\n",
    "        # Define the multivariate normal distribution\n",
    "        print(tf.matmul(w, z[i]))\n",
    "        mvn = tfd.MultivariateNormalDiag(loc=tf.matmul(w, tf.transpose(z)), scale_diag=s * tf.ones([D]))\n",
    "        \n",
    "        # Sample from the distribution\n",
    "        obs = mvn.sample()\n",
    "        \n",
    "        # Compute the log probability of the observation under the distribution\n",
    "        log_prob = mvn.log_prob(obs)\n",
    "        \n",
    "        # Append the observation and its log probability\n",
    "        lik.append(obs)\n",
    "        proba_lik *= tf.exp(log_prob)\n",
    "\n",
    "    # Convert proba_lik to a scalar value\n",
    "    proba_lik = tf.squeeze(proba_lik)\n",
    "    print(\"proba_lik: \", proba_lik)\n",
    "\n",
    "    '''  \n",
    "    print(\"proba_lik: \",proba_lik)\n",
    "    priors = [z,alpha,sigma,w, lik]\n",
    "    proba_priors = [proba_z, proba_alpha, proba_sigma, proba_w, proba_lik]\n",
    "\n",
    "    #joint distribution \n",
    "    p_theta = proba_z*proba_alpha*proba_sigma*proba_w\n",
    "    jd = p_theta*proba_lik\n",
    "    print(\"p_jd: \",jd)'''\n",
    "    pass "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\nafis\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tensorflow_probability\\python\\distributions\\distribution.py:342: MultivariateNormalFullCovariance.__init__ (from tensorflow_probability.python.distributions.mvn_full_covariance) is deprecated and will be removed after 2019-12-01.\n",
      "Instructions for updating:\n",
      "`MultivariateNormalFullCovariance` is deprecated, use `MultivariateNormalTriL(loc=loc, scale_tril=tf.linalg.cholesky(covariance_matrix))` instead.\n",
      "p_z:  3.408052e-26 3.408052e-26\n",
      "p_alpha:  1.9988995e-06\n",
      "p_sigma:  0.33012906\n",
      "proba_w:  1.0669323e-13\n"
     ]
    },
    {
     "ename": "InvalidArgumentError",
     "evalue": "{{function_node __wrapped____MklMatMul_device_/job:localhost/replica:0/task:0/device:CPU:0}} In[1] ndims must be >= 2 [Op:MatMul] name: ",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mprobas\u001b[49m\u001b[43m(\u001b[49m\u001b[43mN\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mM\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mD\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[2], line 43\u001b[0m, in \u001b[0;36mprobas\u001b[1;34m(N, M, D)\u001b[0m\n\u001b[0;32m     40\u001b[0m proba_lik \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m     41\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(N):\n\u001b[0;32m     42\u001b[0m     \u001b[38;5;66;03m# Define the multivariate normal distribution\u001b[39;00m\n\u001b[1;32m---> 43\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[43mtf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmatmul\u001b[49m\u001b[43m(\u001b[49m\u001b[43mw\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mz\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m     44\u001b[0m     mvn \u001b[38;5;241m=\u001b[39m tfd\u001b[38;5;241m.\u001b[39mMultivariateNormalDiag(loc\u001b[38;5;241m=\u001b[39mtf\u001b[38;5;241m.\u001b[39mmatmul(w, tf\u001b[38;5;241m.\u001b[39mtranspose(z)), scale_diag\u001b[38;5;241m=\u001b[39ms \u001b[38;5;241m*\u001b[39m tf\u001b[38;5;241m.\u001b[39mones([D]))\n\u001b[0;32m     46\u001b[0m     \u001b[38;5;66;03m# Sample from the distribution\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\nafis\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tensorflow\\python\\ops\\weak_tensor_ops.py:142\u001b[0m, in \u001b[0;36mweak_tensor_binary_op_wrapper.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    140\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrapper\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m    141\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m ops\u001b[38;5;241m.\u001b[39mis_auto_dtype_conversion_enabled():\n\u001b[1;32m--> 142\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mop\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    143\u001b[0m   bound_arguments \u001b[38;5;241m=\u001b[39m signature\u001b[38;5;241m.\u001b[39mbind(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    144\u001b[0m   bound_arguments\u001b[38;5;241m.\u001b[39mapply_defaults()\n",
      "File \u001b[1;32mc:\\Users\\nafis\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tensorflow\\python\\util\\traceback_utils.py:153\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[1;32m--> 153\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28mNone\u001b[39m\n\u001b[0;32m    154\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    155\u001b[0m   \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32mc:\\Users\\nafis\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tensorflow\\python\\framework\\ops.py:5883\u001b[0m, in \u001b[0;36mraise_from_not_ok_status\u001b[1;34m(e, name)\u001b[0m\n\u001b[0;32m   5881\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mraise_from_not_ok_status\u001b[39m(e, name) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m NoReturn:\n\u001b[0;32m   5882\u001b[0m   e\u001b[38;5;241m.\u001b[39mmessage \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m name: \u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mstr\u001b[39m(name \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n\u001b[1;32m-> 5883\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_status_to_exception(e) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28mNone\u001b[39m\n",
      "\u001b[1;31mInvalidArgumentError\u001b[0m: {{function_node __wrapped____MklMatMul_device_/job:localhost/replica:0/task:0/device:CPU:0}} In[1] ndims must be >= 2 [Op:MatMul] name: "
     ]
    }
   ],
   "source": [
    "probas(N, M, D)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def on_process(n_samples, data, latent_vars): \n",
    "    for s in range(n_samples):\n",
    "    # Form dictionary in order to replace conditioning on prior or\n",
    "    # observed variable with conditioning on a specific value.\n",
    "    dict_swap = {}\n",
    "    for qx in data:\n",
    "        qx_copy = copy(qx) #resample from qx distribution \n",
    "        dict_swap[x] = qx_copy.value()\n",
    "\n",
    "    for qz in latent_vars:\n",
    "      # Copy q(z) to obtain new set of posterior samples.\n",
    "      qz_copy = copy(qz, scope=scope) #resample from qz distribution\n",
    "      dict_swap[z] = qz_copy.value()\n",
    "      q_log_prob[s] += tf.reduce_sum(inference.scale.get(z, 1.0) * qz_copy.log_prob(tf.stop_gradient(dict_swap[z])))\n",
    "\n",
    "    for z in six.iterkeys(inference.latent_vars):\n",
    "      z_copy = copy(z, dict_swap, scope=scope)\n",
    "      p_log_prob[s] += tf.reduce_sum(\n",
    "          inference.scale.get(z, 1.0) * z_copy.log_prob(dict_swap[z]))\n",
    "\n",
    "    for x in six.iterkeys(inference.data):\n",
    "      if isinstance(x, RandomVariable):\n",
    "        x_copy = copy(x, dict_swap, scope=scope)\n",
    "        p_log_prob[s] += tf.reduce_sum(\n",
    "            inference.scale.get(x, 1.0) * x_copy.log_prob(dict_swap[x]))\n",
    "\n",
    "  p_log_prob = tf.stack(p_log_prob)\n",
    "  q_log_prob = tf.stack(q_log_prob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def __init__(self, latent_vars=None, data=None):\n",
    "    \"\"\"Create an inference algorithm.\n",
    "\n",
    "    Args:\n",
    "      latent_vars: list of RandomVariable or\n",
    "                   dict of RandomVariable to RandomVariable.\n",
    "        Collection of random variables to perform inference on. If\n",
    "        list, each random variable will be implictly optimized using a\n",
    "        `Normal` random variable that is defined internally with a\n",
    "        free parameter per location and scale and is initialized using\n",
    "        standard normal draws. The random variables to approximate\n",
    "        must be continuous.\n",
    "    \"\"\"\n",
    "    if isinstance(latent_vars, list):\n",
    "      with tf.variable_scope(None, default_name=\"posterior\"):\n",
    "        latent_vars_dict = {}\n",
    "        continuous = \\\n",
    "            ('01', 'nonnegative', 'simplex', 'real', 'multivariate_real')\n",
    "        for z in latent_vars:\n",
    "          if not hasattr(z, 'support') or z.support not in continuous:\n",
    "            raise AttributeError(\n",
    "                \"Random variable {} is not continuous or a random \"\n",
    "                \"variable with supported continuous support.\".format(z))\n",
    "          batch_event_shape = z.batch_shape.concatenate(z.event_shape)\n",
    "          loc = tf.Variable(tf.random_normal(batch_event_shape))\n",
    "          scale = tf.nn.softplus(\n",
    "              tf.Variable(tf.random_normal(batch_event_shape)))\n",
    "          latent_vars_dict[z] = Normal(loc=loc, scale=scale)\n",
    "        latent_vars = latent_vars_dict\n",
    "        del latent_vars_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import six\n",
    "import tensorflow as tf\n",
    "\n",
    "from edward.inferences.variational_inference import VariationalInference\n",
    "from edward.models import RandomVariable\n",
    "from edward.util import copy, get_descendants\n",
    "\n",
    "\n",
    "  ild_reparam_kl_loss_and_gradients(self, var_list)\n",
    "\n",
    "\n",
    "class ReparameterizationEntropyKLqp(VariationalInference):\n",
    "  \"\"\"Variational inference with the KL divergence\n",
    "\n",
    "  $\\\\text{KL}( q(z; \\lambda) \\| p(z \\mid x) ).$\n",
    "\n",
    "  This class minimizes the objective using the reparameterization\n",
    "  gradient and an analytic entropy term.\n",
    "\n",
    "  The objective function also adds to itself a summation over all\n",
    "  tensors in the `REGULARIZATION_LOSSES` collection.\n",
    "  \"\"\"\n",
    "  def __init__(self, latent_vars=None, data=None):\n",
    "    \"\"\"Create an inference algorithm.\n",
    "\n",
    "    Args:\n",
    "      latent_vars: list of RandomVariable or\n",
    "                   dict of RandomVariable to RandomVariable.\n",
    "        Collection of random variables to perform inference on. If\n",
    "        list, each random variable will be implictly optimized using a\n",
    "        `Normal` random variable that is defined internally with a\n",
    "        free parameter per location and scale and is initialized using\n",
    "        standard normal draws. The random variables to approximate\n",
    "        must be continuous.\n",
    "    \"\"\"\n",
    "    if isinstance(latent_vars, list):\n",
    "      with tf.variable_scope(None, default_name=\"posterior\"):\n",
    "        latent_vars_dict = {}\n",
    "        continuous = \\\n",
    "            ('01', 'nonnegative', 'simplex', 'real', 'multivariate_real')\n",
    "        for z in latent_vars:\n",
    "          if not hasattr(z, 'support') or z.support not in continuous:\n",
    "            raise AttributeError(\n",
    "                \"Random variable {} is not continuous or a random \"\n",
    "                \"variable with supported continuous support.\".format(z))\n",
    "          batch_event_shape = z.batch_shape.concatenate(z.event_shape)\n",
    "          loc = tf.Variable(tf.random_normal(batch_event_shape))\n",
    "          scale = tf.nn.softplus(\n",
    "              tf.Variable(tf.random_normal(batch_event_shape)))\n",
    "          latent_vars_dict[z] = Normal(loc=loc, scale=scale)\n",
    "        latent_vars = latent_vars_dict\n",
    "        del latent_vars_dict\n",
    "\n",
    "    super(ReparameterizationEntropyKLqp, self).__init__(latent_vars, data)\n",
    "\n",
    "  def initialize(self, n_samples=1, *args, **kwargs):\n",
    "    \"\"\"Initialize inference algorithm. It initializes hyperparameters\n",
    "    and builds ops for the algorithm's computation graph.\n",
    "\n",
    "    Args:\n",
    "      n_samples: int.\n",
    "        Number of samples from variational model for calculating\n",
    "        stochastic gradients.\n",
    "    \"\"\"\n",
    "    if n_samples <= 0:\n",
    "      raise ValueError(\n",
    "          \"n_samples should be greater than zero: {}\".format(n_samples))\n",
    "    self.n_samples = n_samples\n",
    "    return super(ReparameterizationEntropyKLqp, self).initialize(\n",
    "        *args, **kwargs)\n",
    "\n",
    "\n",
    "class ScoreKLqp(VariationalInference):\n",
    "  \"\"\"Variational inference with the KL divergence\n",
    "\n",
    "  $\\\\text{KL}( q(z; \\lambda) \\| p(z \\mid x) ).$\n",
    "\n",
    "  This class minimizes the objective using the score function\n",
    "  gradient.\n",
    "\n",
    "  The objective function also adds to itself a summation over all\n",
    "  tensors in the `REGULARIZATION_LOSSES` collection.\n",
    "  \"\"\"\n",
    "  def __init__(self, latent_vars=None, data=None):\n",
    "    \"\"\"Create an inference algorithm.\n",
    "\n",
    "    Args:\n",
    "      latent_vars: list of RandomVariable or\n",
    "                   dict of RandomVariable to RandomVariable.\n",
    "        Collection of random variables to perform inference on. If\n",
    "        list, each random variable will be implictly optimized using a\n",
    "        `Normal` random variable that is defined internally with a\n",
    "        free parameter per location and scale and is initialized using\n",
    "        standard normal draws. The random variables to approximate\n",
    "        must be continuous.\n",
    "    \"\"\"\n",
    "    if isinstance(latent_vars, list):\n",
    "      with tf.variable_scope(None, default_name=\"posterior\"):\n",
    "        latent_vars_dict = {}\n",
    "        continuous = \\\n",
    "            ('01', 'nonnegative', 'simplex', 'real', 'multivariate_real')\n",
    "        for z in latent_vars:\n",
    "          if not hasattr(z, 'support') or z.support not in continuous:\n",
    "            raise AttributeError(\n",
    "                \"Random variable {} is not continuous or a random \"\n",
    "                \"variable with supported continuous support.\".format(z))\n",
    "          batch_event_shape = z.batch_shape.concatenate(z.event_shape)\n",
    "          loc = tf.Variable(tf.random_normal(batch_event_shape))\n",
    "          scale = tf.nn.softplus(\n",
    "              tf.Variable(tf.random_normal(batch_event_shape)))\n",
    "          latent_vars_dict[z] = Normal(loc=loc, scale=scale)\n",
    "        latent_vars = latent_vars_dict\n",
    "        del latent_vars_dict\n",
    "\n",
    "    super(ScoreKLqp, self).__init__(latent_vars, data)\n",
    "\n",
    "  def initialize(self, n_samples=1, *args, **kwargs):\n",
    "    \"\"\"Initialize inference algorithm. It initializes hyperparameters\n",
    "    and builds ops for the algorithm's computation graph.\n",
    "\n",
    "    Args:\n",
    "      n_samples: int.\n",
    "        Number of samples from variational model for calculating\n",
    "        stochastic gradients.\n",
    "    \"\"\"\n",
    "    if n_samples <= 0:\n",
    "      raise ValueError(\n",
    "          \"n_samples should be greater than zero: {}\".format(n_samples))\n",
    "    self.n_samples = n_samples\n",
    "    return super(ScoreKLqp, self).initialize(*args, **kwargs)\n",
    "\n",
    "  def build_loss_and_gradients(self, var_list):\n",
    "    return build_score_loss_and_gradients(self, var_list)\n",
    "\n",
    "\n",
    "class ScoreKLKLqp(VariationalInference):\n",
    "  \"\"\"Variational inference with the KL divergence\n",
    "\n",
    "  $\\\\text{KL}( q(z; \\lambda) \\| p(z \\mid x) ).$\n",
    "\n",
    "  This class minimizes the objective using the score function gradient\n",
    "  and an analytic KL term.\n",
    "\n",
    "  The objective function also adds to itself a summation over all\n",
    "  tensors in the `REGULARIZATION_LOSSES` collection.\n",
    "  \"\"\"\n",
    "  def __init__(self, latent_vars=None, data=None):\n",
    "    \"\"\"Create an inference algorithm.\n",
    "\n",
    "    Args:\n",
    "      latent_vars: list of RandomVariable or\n",
    "                   dict of RandomVariable to RandomVariable.\n",
    "        Collection of random variables to perform inference on. If\n",
    "        list, each random variable will be implictly optimized using a\n",
    "        `Normal` random variable that is defined internally with a\n",
    "        free parameter per location and scale and is initialized using\n",
    "        standard normal draws. The random variables to approximate\n",
    "        must be continuous.\n",
    "    \"\"\"\n",
    "    if isinstance(latent_vars, list):\n",
    "      with tf.variable_scope(None, default_name=\"posterior\"):\n",
    "        latent_vars_dict = {}\n",
    "        continuous = \\\n",
    "            ('01', 'nonnegative', 'simplex', 'real', 'multivariate_real')\n",
    "        for z in latent_vars:\n",
    "          if not hasattr(z, 'support') or z.support not in continuous:\n",
    "            raise AttributeError(\n",
    "                \"Random variable {} is not continuous or a random \"\n",
    "                \"variable with supported continuous support.\".format(z))\n",
    "          batch_event_shape = z.batch_shape.concatenate(z.event_shape)\n",
    "          loc = tf.Variable(tf.random_normal(batch_event_shape))\n",
    "          scale = tf.nn.softplus(\n",
    "              tf.Variable(tf.random_normal(batch_event_shape)))\n",
    "          latent_vars_dict[z] = Normal(loc=loc, scale=scale)\n",
    "        latent_vars = latent_vars_dict\n",
    "        del latent_vars_dict\n",
    "\n",
    "    super(ScoreKLKLqp, self).__init__(latent_vars, data)\n",
    "\n",
    "  def initialize(self, n_samples=1, kl_scaling=None, *args, **kwargs):\n",
    "    \"\"\"Initialize inference algorithm. It initializes hyperparameters\n",
    "    and builds ops for the algorithm's computation graph.\n",
    "\n",
    "    Args:\n",
    "      n_samples: int.\n",
    "        Number of samples from variational model for calculating\n",
    "        stochastic gradients.\n",
    "      kl_scaling: dict of RandomVariable to tf.Tensor.\n",
    "        Provides option to scale terms when using ELBO with KL divergence.\n",
    "        If the KL divergence terms are\n",
    "\n",
    "        $\\\\alpha_p \\mathbb{E}_{q(z\\mid x, \\lambda)} [\n",
    "              \\log q(z\\mid x, \\lambda) - \\log p(z)],$\n",
    "\n",
    "        then pass {$p(z)$: $\\\\alpha_p$} as `kl_scaling`,\n",
    "        where $\\\\alpha_p$ is a tensor. Its shape must be broadcastable;\n",
    "        it is multiplied element-wise to the batchwise KL terms.\n",
    "    \"\"\"\n",
    "    if kl_scaling is None:\n",
    "      kl_scaling = {}\n",
    "    if n_samples <= 0:\n",
    "      raise ValueError(\n",
    "          \"n_samples should be greater than zero: {}\".format(n_samples))\n",
    "    self.n_samples = n_samples\n",
    "    self.kl_scaling = kl_scaling\n",
    "    return super(ScoreKLKLqp, self).initialize(*args, **kwargs)\n",
    "\n",
    "  def build_loss_and_gradients(self, var_list):\n",
    "    return build_score_kl_loss_and_gradients(self, var_list)\n",
    "\n",
    "\n",
    "class ScoreEntropyKLqp(VariationalInference):\n",
    "  \"\"\"Variational inference with the KL divergence\n",
    "\n",
    "  $\\\\text{KL}( q(z; \\lambda) \\| p(z \\mid x) ).$\n",
    "\n",
    "  This class minimizes the objective using the score function gradient\n",
    "  and an analytic entropy term.\n",
    "\n",
    "  The objective function also adds to itself a summation over all\n",
    "  tensors in the `REGULARIZATION_LOSSES` collection.\n",
    "  \"\"\"\n",
    "  def __init__(self, latent_vars=None, data=None):\n",
    "    \"\"\"Create an inference algorithm.\n",
    "\n",
    "    Args:\n",
    "      latent_vars: list of RandomVariable or\n",
    "                   dict of RandomVariable to RandomVariable.\n",
    "        Collection of random variables to perform inference on. If\n",
    "        list, each random variable will be implictly optimized using a\n",
    "        `Normal` random variable that is defined internally with a\n",
    "        free parameter per location and scale and is initialized using\n",
    "        standard normal draws. The random variables to approximate\n",
    "        must be continuous.\n",
    "    \"\"\"\n",
    "    if isinstance(latent_vars, list):\n",
    "      with tf.variable_scope(None, default_name=\"posterior\"):\n",
    "        latent_vars_dict = {}\n",
    "        continuous = \\\n",
    "            ('01', 'nonnegative', 'simplex', 'real', 'multivariate_real')\n",
    "        for z in latent_vars:\n",
    "          if not hasattr(z, 'support') or z.support not in continuous:\n",
    "            raise AttributeError(\n",
    "                \"Random variable {} is not continuous or a random \"\n",
    "                \"variable with supported continuous support.\".format(z))\n",
    "          batch_event_shape = z.batch_shape.concatenate(z.event_shape)\n",
    "          loc = tf.Variable(tf.random_normal(batch_event_shape))\n",
    "          scale = tf.nn.softplus(\n",
    "              tf.Variable(tf.random_normal(batch_event_shape)))\n",
    "          latent_vars_dict[z] = Normal(loc=loc, scale=scale)\n",
    "        latent_vars = latent_vars_dict\n",
    "        del latent_vars_dict\n",
    "\n",
    "    super(ScoreEntropyKLqp, self).__init__(latent_vars, data)\n",
    "\n",
    "  def initialize(self, n_samples=1, *args, **kwargs):\n",
    "    \"\"\"Initialize inference algorithm. It initializes hyperparameters\n",
    "    and builds ops for the algorithm's computation graph.\n",
    "\n",
    "    Args:\n",
    "      n_samples: int.\n",
    "        Number of samples from variational model for calculating\n",
    "        stochastic gradients.\n",
    "    \"\"\"\n",
    "    if n_samples <= 0:\n",
    "      raise ValueError(\n",
    "          \"n_samples should be greater than zero: {}\".format(n_samples))\n",
    "    self.n_samples = n_samples\n",
    "    return super(ScoreEntropyKLqp, self).initialize(*args, **kwargs)\n",
    "\n",
    "  def build_loss_and_gradients(self, var_list):\n",
    "    return build_score_entropy_loss_and_gradients(self, var_list)\n",
    "\n",
    "\n",
    "\n",
    "def build_reparam_loss_and_gradients(inference, var_list):\n",
    "  \"\"\"Build loss function. Its automatic differentiation\n",
    "  is a stochastic gradient of\n",
    "\n",
    "  $-\\\\text{ELBO} =\n",
    "      -\\mathbb{E}_{q(z; \\lambda)} [ \\log p(x, z) - \\log q(z; \\lambda) ]$\n",
    "\n",
    "  based on the reparameterization trick [@kingma2014auto].\n",
    "\n",
    "  Computed by sampling from $q(z;\\lambda)$ and evaluating the\n",
    "  expectation using Monte Carlo sampling.\n",
    "  \"\"\"\n",
    "  p_log_prob = [0.0] * inference.n_samples\n",
    "  q_log_prob = [0.0] * inference.n_samples\n",
    "  base_scope = tf.get_default_graph().unique_name(\"inference\") + '/'\n",
    "  for s in range(inference.n_samples):\n",
    "    # Form dictionary in order to replace conditioning on prior or\n",
    "    # observed variable with conditioning on a specific value.\n",
    "    scope = base_scope + tf.get_default_graph().unique_name(\"sample\")\n",
    "    dict_swap = {}\n",
    "    for x, qx in six.iteritems(inference.data):\n",
    "      if isinstance(x, RandomVariable):\n",
    "        if isinstance(qx, RandomVariable):\n",
    "          qx_copy = copy(qx, scope=scope)\n",
    "          dict_swap[x] = qx_copy.value()\n",
    "        else:\n",
    "          dict_swap[x] = qx\n",
    "\n",
    "    for z, qz in six.iteritems(inference.latent_vars):\n",
    "      # Copy q(z) to obtain new set of posterior samples.\n",
    "      qz_copy = copy(qz, scope=scope)\n",
    "      dict_swap[z] = qz_copy.value()\n",
    "      q_log_prob[s] += tf.reduce_sum(\n",
    "          inference.scale.get(z, 1.0) * qz_copy.log_prob(dict_swap[z]))\n",
    "\n",
    "    for z in six.iterkeys(inference.latent_vars):\n",
    "      z_copy = copy(z, dict_swap, scope=scope)\n",
    "      p_log_prob[s] += tf.reduce_sum(\n",
    "          inference.scale.get(z, 1.0) * z_copy.log_prob(dict_swap[z]))\n",
    "\n",
    "    for x in six.iterkeys(inference.data):\n",
    "      if isinstance(x, RandomVariable):\n",
    "        x_copy = copy(x, dict_swap, scope=scope)\n",
    "        p_log_prob[s] += tf.reduce_sum(\n",
    "            inference.scale.get(x, 1.0) * x_copy.log_prob(dict_swap[x]))\n",
    "\n",
    "  p_log_prob = tf.reduce_mean(p_log_prob)\n",
    "  q_log_prob = tf.reduce_mean(q_log_prob)\n",
    "  reg_penalty = tf.reduce_sum(tf.losses.get_regularization_losses())\n",
    "\n",
    "  if inference.logging:\n",
    "    tf.summary.scalar(\"loss/p_log_prob\", p_log_prob,\n",
    "                      collections=[inference._summary_key])\n",
    "    tf.summary.scalar(\"loss/q_log_prob\", q_log_prob,\n",
    "                      collections=[inference._summary_key])\n",
    "    tf.summary.scalar(\"loss/reg_penalty\", reg_penalty,\n",
    "                      collections=[inference._summary_key])\n",
    "\n",
    "  loss = -(p_log_prob - q_log_prob - reg_penalty)\n",
    "\n",
    "  grads = tf.gradients(loss, var_list)\n",
    "  grads_and_vars = list(zip(grads, var_list))\n",
    "  return loss, grads_and_vars\n",
    "\n",
    "\n",
    "def build_reparam_kl_loss_and_gradients(inference, var_list):\n",
    "  \"\"\"Build loss function. Its automatic differentiation\n",
    "  is a stochastic gradient of\n",
    "\n",
    "  .. math::\n",
    "\n",
    "    -\\\\text{ELBO} =  - ( \\mathbb{E}_{q(z; \\lambda)} [ \\log p(x \\mid z) ]\n",
    "          + \\\\text{KL}(q(z; \\lambda) \\| p(z)) )\n",
    "\n",
    "  based on the reparameterization trick [@kingma2014auto].\n",
    "\n",
    "  It assumes the KL is analytic.\n",
    "\n",
    "  Computed by sampling from $q(z;\\lambda)$ and evaluating the\n",
    "  expectation using Monte Carlo sampling.\n",
    "  \"\"\"\n",
    "  p_log_lik = [0.0] * inference.n_samples\n",
    "  base_scope = tf.get_default_graph().unique_name(\"inference\") + '/'\n",
    "  for s in range(inference.n_samples):\n",
    "    # Form dictionary in order to replace conditioning on prior or\n",
    "    # observed variable with conditioning on a specific value.\n",
    "    scope = base_scope + tf.get_default_graph().unique_name(\"sample\")\n",
    "    dict_swap = {}\n",
    "    for x, qx in six.iteritems(inference.data):\n",
    "      if isinstance(x, RandomVariable):\n",
    "        if isinstance(qx, RandomVariable):\n",
    "          qx_copy = copy(qx, scope=scope)\n",
    "          dict_swap[x] = qx_copy.value()\n",
    "        else:\n",
    "          dict_swap[x] = qx\n",
    "\n",
    "    for z, qz in six.iteritems(inference.latent_vars):\n",
    "      # Copy q(z) to obtain new set of posterior samples.\n",
    "      qz_copy = copy(qz, scope=scope)\n",
    "      dict_swap[z] = qz_copy.value()\n",
    "\n",
    "    for x in six.iterkeys(inference.data):\n",
    "      if isinstance(x, RandomVariable):\n",
    "        x_copy = copy(x, dict_swap, scope=scope)\n",
    "        p_log_lik[s] += tf.reduce_sum(\n",
    "            inference.scale.get(x, 1.0) * x_copy.log_prob(dict_swap[x]))\n",
    "\n",
    "  p_log_lik = tf.reduce_mean(p_log_lik)\n",
    "\n",
    "  kl_penalty = tf.reduce_sum([\n",
    "      tf.reduce_sum(inference.kl_scaling.get(z, 1.0) * kl_divergence(qz, z))\n",
    "      for z, qz in six.iteritems(inference.latent_vars)])\n",
    "\n",
    "  reg_penalty = tf.reduce_sum(tf.losses.get_regularization_losses())\n",
    "\n",
    "  if inference.logging:\n",
    "    tf.summary.scalar(\"loss/p_log_lik\", p_log_lik,\n",
    "                      collections=[inference._summary_key])\n",
    "    tf.summary.scalar(\"loss/kl_penalty\", kl_penalty,\n",
    "                      collections=[inference._summary_key])\n",
    "    tf.summary.scalar(\"loss/reg_penalty\", reg_penalty,\n",
    "                      collections=[inference._summary_key])\n",
    "\n",
    "  loss = -(p_log_lik - kl_penalty - reg_penalty)\n",
    "\n",
    "  grads = tf.gradients(loss, var_list)\n",
    "  grads_and_vars = list(zip(grads, var_list))\n",
    "  return loss, grads_and_vars\n",
    "\n",
    "def build_score_loss_and_gradients(inference, var_list):\n",
    "  \"\"\"Build loss function and gradients based on the score function\n",
    "  estimator [@paisley2012variational].\n",
    "\n",
    "  Computed by sampling from $q(z;\\lambda)$ and evaluating the\n",
    "  expectation using Monte Carlo sampling.\n",
    "  \"\"\"\n",
    "  p_log_prob = [0.0] * inference.n_samples\n",
    "  q_log_prob = [0.0] * inference.n_samples\n",
    "  base_scope = tf.get_default_graph().unique_name(\"inference\") + '/'\n",
    "  for s in range(inference.n_samples):\n",
    "    # Form dictionary in order to replace conditioning on prior or\n",
    "    # observed variable with conditioning on a specific value.\n",
    "    scope = base_scope + tf.get_default_graph().unique_name(\"sample\")\n",
    "    dict_swap = {}\n",
    "    for x, qx in six.iteritems(inference.data):\n",
    "      if isinstance(x, RandomVariable):\n",
    "        if isinstance(qx, RandomVariable):\n",
    "          qx_copy = copy(qx, scope=scope)\n",
    "          dict_swap[x] = qx_copy.value()\n",
    "        else:\n",
    "          dict_swap[x] = qx\n",
    "\n",
    "    for z, qz in six.iteritems(inference.latent_vars):\n",
    "      # Copy q(z) to obtain new set of posterior samples.\n",
    "      qz_copy = copy(qz, scope=scope)\n",
    "      dict_swap[z] = qz_copy.value()\n",
    "      q_log_prob[s] += tf.reduce_sum(\n",
    "          inference.scale.get(z, 1.0) *\n",
    "          qz_copy.log_prob(tf.stop_gradient(dict_swap[z])))\n",
    "\n",
    "    for z in six.iterkeys(inference.latent_vars):\n",
    "      z_copy = copy(z, dict_swap, scope=scope)\n",
    "      p_log_prob[s] += tf.reduce_sum(\n",
    "          inference.scale.get(z, 1.0) * z_copy.log_prob(dict_swap[z]))\n",
    "\n",
    "    for x in six.iterkeys(inference.data):\n",
    "      if isinstance(x, RandomVariable):\n",
    "        x_copy = copy(x, dict_swap, scope=scope)\n",
    "        p_log_prob[s] += tf.reduce_sum(\n",
    "            inference.scale.get(x, 1.0) * x_copy.log_prob(dict_swap[x]))\n",
    "\n",
    "  p_log_prob = tf.stack(p_log_prob)\n",
    "  q_log_prob = tf.stack(q_log_prob)\n",
    "  reg_penalty = tf.reduce_sum(tf.losses.get_regularization_losses())\n",
    "\n",
    "  if inference.logging:\n",
    "    tf.summary.scalar(\"loss/p_log_prob\", tf.reduce_mean(p_log_prob),\n",
    "                      collections=[inference._summary_key])\n",
    "    tf.summary.scalar(\"loss/q_log_prob\", tf.reduce_mean(q_log_prob),\n",
    "                      collections=[inference._summary_key])\n",
    "    tf.summary.scalar(\"loss/reg_penalty\", reg_penalty,\n",
    "                      collections=[inference._summary_key])\n",
    "\n",
    "  losses = p_log_prob - q_log_prob\n",
    "  loss = -(tf.reduce_mean(losses) - reg_penalty)\n",
    "\n",
    "  q_rvs = list(six.itervalues(inference.latent_vars))\n",
    "  q_vars = [v for v in var_list\n",
    "            if len(get_descendants(tf.convert_to_tensor(v), q_rvs)) != 0]\n",
    "  q_grads = tf.gradients(\n",
    "      -(tf.reduce_mean(q_log_prob * tf.stop_gradient(losses)) - reg_penalty),\n",
    "      q_vars)\n",
    "  p_vars = [v for v in var_list if v not in q_vars]\n",
    "  p_grads = tf.gradients(loss, p_vars)\n",
    "  grads_and_vars = list(zip(q_grads, q_vars)) + list(zip(p_grads, p_vars))\n",
    "  return loss, grads_and_vars"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
